#!/usr/bin/env python3
"""
Goé¡¹ç›®å¤æ‚åº¦åˆ†æå·¥å…· - æ›´æ™ºèƒ½çš„é¡¹ç›®å¤æ‚åº¦è¯„ä¼°
æ’é™¤ç¬¬ä¸‰æ–¹ä»£ç ï¼Œä½¿ç”¨è®¤çŸ¥å¤æ‚åº¦ã€åœˆå¤æ‚åº¦ç­‰æ›´å¥½çš„æŒ‡æ ‡
"""

import os
import re
import ast
import pathlib
from collections import defaultdict, Counter
from dataclasses import dataclass
from typing import List, Dict, Set, Tuple, Optional


@dataclass
class FunctionInfo:
    name: str
    line_start: int
    line_end: int
    cyclomatic_complexity: int
    cognitive_complexity: int
    nesting_depth: int
    parameters: int
    return_values: int


@dataclass
class FileComplexity:
    path: str
    total_lines: int
    code_lines: int
    comment_lines: int
    empty_lines: int
    functions: List[FunctionInfo]
    structs_count: int
    interfaces_count: int
    imports_count: int
    max_nesting_depth: int
    avg_function_length: float
    is_test: bool
    is_generated: bool
    is_third_party: bool


class GoComplexityAnalyzer:
    def __init__(self, root_dir='.'):
        self.root_dir = root_dir
        self.third_party_patterns = [
            'vendor/',
            '.git/',
            'node_modules/',
            # å¸¸è§çš„è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶æ¨¡å¼
            r'.*\.pb\.go$',
            r'.*_generated\.go$',
            r'mock_.*\.go$',
        ]
        
        # æ’é™¤ä¸é‡è¦çš„ç›®å½•
        self.exclude_dirs = {
            '.git', 'vendor', 'node_modules', '.vscode', 
            '.idea', 'tmp', 'temp', 'build', 'dist'
        }

    def is_third_party_or_generated(self, file_path: str) -> tuple[bool, bool]:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ˜¯ç¬¬ä¸‰æ–¹ä»£ç æˆ–è‡ªåŠ¨ç”Ÿæˆçš„ä»£ç """
        rel_path = os.path.relpath(file_path, self.root_dir)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤ç›®å½•ä¸­
        path_parts = rel_path.split(os.sep)
        for part in path_parts:
            if part in self.exclude_dirs:
                return True, False
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç¬¬ä¸‰æ–¹æ¨¡å¼
        for pattern in self.third_party_patterns:
            if pattern.endswith('/'):
                if rel_path.startswith(pattern):
                    return True, False
            elif re.match(pattern, rel_path):
                return True, False
        
        # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦åŒ…å«ç”Ÿæˆæ ‡è®°
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                first_lines = f.read(500)  # åªè¯»å‰500å­—ç¬¦
                if any(marker in first_lines for marker in [
                    'Code generated',
                    'DO NOT EDIT',
                    'AUTO-GENERATED',
                    'This file was autogenerated'
                ]):
                    return False, True
        except:
            pass
        
        return False, False

    def calculate_cyclomatic_complexity(self, content: str) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦"""
        # åŸºç¡€å¤æ‚åº¦ä¸º1
        complexity = 1
        
        # å¢åŠ å¤æ‚åº¦çš„Goè¯­è¨€ç»“æ„
        complexity_patterns = [
            r'\bif\b',
            r'\belse\s+if\b',
            r'\bfor\b',
            r'\bswitch\b',
            r'\bcase\b',
            r'\bselect\b',
            r'\bgo\b\s+\w+\s*\(',  # goroutine
            r'\bdefer\b',
            r'&&',
            r'\|\|',
            r'\?.*:',  # ä¸‰å…ƒæ“ä½œç¬¦
        ]
        
        for pattern in complexity_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            complexity += len(matches)
        
        return complexity

    def calculate_cognitive_complexity(self, content: str) -> int:
        """è®¡ç®—è®¤çŸ¥å¤æ‚åº¦ï¼ˆæ›´æ¥è¿‘äººç±»ç†è§£ä»£ç çš„éš¾åº¦ï¼‰"""
        cognitive = 0
        nesting_level = 0
        
        lines = content.split('\n')
        
        for line in lines:
            stripped = line.strip()
            
            # è®¡ç®—åµŒå¥—çº§åˆ«
            if any(keyword in stripped for keyword in ['if', 'for', 'switch', 'select']):
                cognitive += 1 + nesting_level  # åŸºç¡€å¤æ‚åº¦ + åµŒå¥—æƒ©ç½š
                if '{' in stripped:
                    nesting_level += 1
            elif stripped.startswith('case ') or stripped.startswith('default:'):
                cognitive += 1
            elif '&&' in stripped or '||' in stripped:
                cognitive += 1
            elif stripped.startswith('go '):  # goroutine
                cognitive += 1
            elif stripped.startswith('defer '):
                cognitive += 1
            
            # æ£€æŸ¥å—ç»“æŸ
            if '}' in stripped:
                nesting_level = max(0, nesting_level - 1)
        
        return cognitive

    def calculate_nesting_depth(self, content: str) -> int:
        """è®¡ç®—æœ€å¤§åµŒå¥—æ·±åº¦"""
        max_depth = 0
        current_depth = 0
        
        for char in content:
            if char == '{':
                current_depth += 1
                max_depth = max(max_depth, current_depth)
            elif char == '}':
                current_depth = max(0, current_depth - 1)
        
        return max_depth

    def extract_function_info(self, content: str) -> List[FunctionInfo]:
        """æå–å‡½æ•°ä¿¡æ¯"""
        functions = []
        lines = content.split('\n')
        
        # ç®€å•çš„Goå‡½æ•°åŒ¹é…
        func_pattern = r'func\s+(?:\([^)]*\)\s+)?(\w+)\s*\(([^)]*)\)\s*(?:\([^)]*\))?\s*(?:\w+\s*)?{'
        
        for i, line in enumerate(lines):
            match = re.search(func_pattern, line)
            if match:
                func_name = match.group(1)
                params = match.group(2) if match.group(2) else ""
                
                # è®¡ç®—å‚æ•°æ•°é‡
                param_count = len([p.strip() for p in params.split(',') if p.strip()]) if params.strip() else 0
                
                # æ‰¾åˆ°å‡½æ•°ç»“æŸä½ç½®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                brace_count = 0
                func_start = i + 1
                func_end = func_start
                started = False
                
                for j in range(i, len(lines)):
                    for char in lines[j]:
                        if char == '{':
                            brace_count += 1
                            started = True
                        elif char == '}':
                            brace_count -= 1
                            
                        if started and brace_count == 0:
                            func_end = j + 1
                            break
                    if started and brace_count == 0:
                        break
                
                # æå–å‡½æ•°å†…å®¹
                func_content = '\n'.join(lines[i:func_end])
                
                functions.append(FunctionInfo(
                    name=func_name,
                    line_start=func_start,
                    line_end=func_end,
                    cyclomatic_complexity=self.calculate_cyclomatic_complexity(func_content),
                    cognitive_complexity=self.calculate_cognitive_complexity(func_content),
                    nesting_depth=self.calculate_nesting_depth(func_content),
                    parameters=param_count,
                    return_values=0  # ç®€åŒ–ï¼Œä¸è§£æè¿”å›å€¼
                ))
        
        return functions

    def analyze_file(self, file_path: str) -> FileComplexity:
        """åˆ†æå•ä¸ªGoæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                lines = content.split('\n')
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
            return None

        # åŸºç¡€ç»Ÿè®¡
        total_lines = len(lines)
        empty_lines = sum(1 for line in lines if line.strip() == '')
        comment_lines = sum(1 for line in lines if line.strip().startswith('//'))
        code_lines = total_lines - empty_lines - comment_lines

        # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬ä¸‰æ–¹æˆ–ç”Ÿæˆçš„ä»£ç 
        is_third_party, is_generated = self.is_third_party_or_generated(file_path)
        
        # æå–å‡½æ•°ä¿¡æ¯
        functions = self.extract_function_info(content)
        
        # ç»Ÿè®¡ç»“æ„ä½“å’Œæ¥å£
        structs_count = len(re.findall(r'type\s+\w+\s+struct\s*{', content))
        interfaces_count = len(re.findall(r'type\s+\w+\s+interface\s*{', content))
        
        # ç»Ÿè®¡å¯¼å…¥
        import_section = re.search(r'import\s*\((.*?)\)', content, re.DOTALL)
        if import_section:
            imports = [line.strip() for line in import_section.group(1).split('\n') if line.strip() and not line.strip().startswith('//')]
            imports_count = len([imp for imp in imports if imp])
        else:
            # å•è¡Œå¯¼å…¥
            imports_count = len(re.findall(r'import\s+"[^"]+"', content))

        # è®¡ç®—å¹³å‡å‡½æ•°é•¿åº¦
        if functions:
            avg_function_length = sum(f.line_end - f.line_start for f in functions) / len(functions)
        else:
            avg_function_length = 0

        return FileComplexity(
            path=file_path,
            total_lines=total_lines,
            code_lines=code_lines,
            comment_lines=comment_lines,
            empty_lines=empty_lines,
            functions=functions,
            structs_count=structs_count,
            interfaces_count=interfaces_count,
            imports_count=imports_count,
            max_nesting_depth=self.calculate_nesting_depth(content),
            avg_function_length=avg_function_length,
            is_test=file_path.endswith('_test.go'),
            is_generated=is_generated,
            is_third_party=is_third_party
        )

    def categorize_files(self, files: List[FileComplexity]) -> Dict[str, List[FileComplexity]]:
        """å°†æ–‡ä»¶åˆ†ç±»"""
        categories = {
            'æ ¸å¿ƒä¸šåŠ¡é€»è¾‘': [],
            'æµ‹è¯•ä»£ç ': [],
            'å·¥å…·å’ŒCLI': [],
            'APIå®šä¹‰': [],
            'é…ç½®å’Œåˆå§‹åŒ–': [],
            'ç¬¬ä¸‰æ–¹/ç”Ÿæˆä»£ç ': []
        }
        
        for file in files:
            if file.is_third_party or file.is_generated:
                categories['ç¬¬ä¸‰æ–¹/ç”Ÿæˆä»£ç '].append(file)
            elif file.is_test:
                categories['æµ‹è¯•ä»£ç '].append(file)
            elif '/cmd/' in file.path:
                categories['å·¥å…·å’ŒCLI'].append(file)
            elif '/api/' in file.path or 'types.go' in file.path:
                categories['APIå®šä¹‰'].append(file)
            elif 'config' in file.path.lower() or 'init' in file.path.lower():
                categories['é…ç½®å’Œåˆå§‹åŒ–'].append(file)
            else:
                categories['æ ¸å¿ƒä¸šåŠ¡é€»è¾‘'].append(file)
        
        return categories

    def analyze_project(self) -> Dict:
        """åˆ†ææ•´ä¸ªé¡¹ç›®"""
        print("ğŸ” æ­£åœ¨æ‰«æGoæ–‡ä»¶...")
        
        go_files = []
        for root, dirs, files in os.walk(self.root_dir):
            # è¿‡æ»¤ç›®å½•
            dirs[:] = [d for d in dirs if d not in self.exclude_dirs]
            
            for file in files:
                if file.endswith('.go'):
                    file_path = os.path.join(root, file)
                    complexity = self.analyze_file(file_path)
                    if complexity:
                        go_files.append(complexity)

        print(f"ğŸ“ æ‰¾åˆ° {len(go_files)} ä¸ªGoæ–‡ä»¶")
        
        # åˆ†ç±»æ–‡ä»¶
        categorized = self.categorize_files(go_files)
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        stats = {}
        for category, files in categorized.items():
            if not files:
                continue
                
            total_code_lines = sum(f.code_lines for f in files)
            total_functions = sum(len(f.functions) for f in files)
            total_complexity = sum(sum(func.cognitive_complexity for func in f.functions) for f in files)
            
            # æ‰¾å‡ºæœ€å¤æ‚çš„å‡½æ•°
            all_functions = []
            for f in files:
                for func in f.functions:
                    all_functions.append((func, f.path))
            
            most_complex = sorted(all_functions, key=lambda x: x[0].cognitive_complexity, reverse=True)[:5]
            
            stats[category] = {
                'files_count': len(files),
                'total_code_lines': total_code_lines,
                'total_functions': total_functions,
                'total_complexity': total_complexity,
                'avg_complexity_per_function': total_complexity / max(total_functions, 1),
                'most_complex_functions': most_complex,
                'files': files
            }
        
        return stats

    def print_analysis(self, stats: Dict):
        """æ‰“å°åˆ†æç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ§  GOé¡¹ç›®å¤æ‚åº¦åˆ†ææŠ¥å‘Š")
        print("="*80)
        
        # æ€»è§ˆ
        total_core_lines = sum(s['total_code_lines'] for k, s in stats.items() 
                              if k not in ['æµ‹è¯•ä»£ç ', 'ç¬¬ä¸‰æ–¹/ç”Ÿæˆä»£ç '])
        total_core_functions = sum(s['total_functions'] for k, s in stats.items() 
                                  if k not in ['æµ‹è¯•ä»£ç ', 'ç¬¬ä¸‰æ–¹/ç”Ÿæˆä»£ç '])
        total_core_complexity = sum(s['total_complexity'] for k, s in stats.items() 
                                   if k not in ['æµ‹è¯•ä»£ç ', 'ç¬¬ä¸‰æ–¹/ç”Ÿæˆä»£ç '])
        
        print(f"\nğŸ“Š æ ¸å¿ƒä»£ç æ¦‚è§ˆ (æ’é™¤æµ‹è¯•å’Œç¬¬ä¸‰æ–¹ä»£ç ):")
        print(f"  â€¢ ä»£ç è¡Œæ•°: {total_core_lines:,}")
        print(f"  â€¢ å‡½æ•°æ•°é‡: {total_core_functions:,}")
        print(f"  â€¢ æ€»è®¤çŸ¥å¤æ‚åº¦: {total_core_complexity:,}")
        print(f"  â€¢ å¹³å‡å‡½æ•°å¤æ‚åº¦: {total_core_complexity/max(total_core_functions,1):.1f}")
        
        # å¤æ‚åº¦ç­‰çº§è¯„ä¼°
        if total_core_complexity / max(total_core_functions, 1) <= 5:
            complexity_level = "ğŸŸ¢ ä½å¤æ‚åº¦ - æ˜“äºç†è§£"
        elif total_core_complexity / max(total_core_functions, 1) <= 10:
            complexity_level = "ğŸŸ¡ ä¸­ç­‰å¤æ‚åº¦ - éœ€è¦ä¸€å®šç»éªŒ"
        elif total_core_complexity / max(total_core_functions, 1) <= 20:
            complexity_level = "ğŸŸ  é«˜å¤æ‚åº¦ - éœ€è¦æ·±åº¦ç†è§£"
        else:
            complexity_level = "ğŸ”´ æé«˜å¤æ‚åº¦ - å»ºè®®é‡æ„"
        
        print(f"\nğŸ¯ é¡¹ç›®å¤æ‚åº¦ç­‰çº§: {complexity_level}")
        
        # æŒ‰ç±»åˆ«è¯¦ç»†åˆ†æ
        print(f"\nğŸ“‚ æŒ‰æ¨¡å—åˆ†ç±»åˆ†æ:")
        print("-" * 70)
        
        for category, data in stats.items():
            if data['files_count'] == 0:
                continue
                
            print(f"\nğŸ”¸ {category}")
            print(f"   æ–‡ä»¶æ•°: {data['files_count']}")
            print(f"   ä»£ç è¡Œ: {data['total_code_lines']:,}")
            print(f"   å‡½æ•°æ•°: {data['total_functions']}")
            print(f"   å¹³å‡å¤æ‚åº¦: {data['avg_complexity_per_function']:.1f}")
            
            # æ˜¾ç¤ºæœ€å¤æ‚çš„å‡½æ•°
            if data['most_complex_functions']:
                print(f"   æœ€å¤æ‚å‡½æ•°:")
                for i, (func, path) in enumerate(data['most_complex_functions'][:3]):
                    rel_path = os.path.relpath(path, self.root_dir)
                    print(f"     {i+1}. {func.name} (å¤æ‚åº¦:{func.cognitive_complexity}) - {rel_path}")
        
        # é‡ç‚¹å…³æ³¨åŒºåŸŸå»ºè®®
        print(f"\nğŸ’¡ å»ºè®®ä¼˜å…ˆå…³æ³¨çš„å­¦ä¹ è·¯å¾„:")
        print("-" * 50)
        
        # æ‰¾å‡ºæ ¸å¿ƒAPIå®šä¹‰
        if 'APIå®šä¹‰' in stats and stats['APIå®šä¹‰']['files']:
            print("1ï¸âƒ£ å…ˆä»APIå®šä¹‰å¼€å§‹:")
            for file in sorted(stats['APIå®šä¹‰']['files'], key=lambda f: f.structs_count + f.interfaces_count, reverse=True)[:3]:
                rel_path = os.path.relpath(file.path, self.root_dir)
                print(f"   ğŸ“‹ {rel_path} ({file.structs_count}ä¸ªç»“æ„ä½“, {file.interfaces_count}ä¸ªæ¥å£)")
        
        # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ä¸­æœ€ç®€å•çš„æ¨¡å—
        if 'æ ¸å¿ƒä¸šåŠ¡é€»è¾‘' in stats and stats['æ ¸å¿ƒä¸šåŠ¡é€»è¾‘']['files']:
            simple_files = sorted(stats['æ ¸å¿ƒä¸šåŠ¡é€»è¾‘']['files'], 
                                key=lambda f: sum(func.cognitive_complexity for func in f.functions))[:5]
            print("\n2ï¸âƒ£ ç„¶åå­¦ä¹ ç®€å•çš„æ ¸å¿ƒæ¨¡å—:")
            for file in simple_files:
                rel_path = os.path.relpath(file.path, self.root_dir)
                total_complexity = sum(func.cognitive_complexity for func in file.functions)
                print(f"   ğŸ§© {rel_path} (å¤æ‚åº¦:{total_complexity}, {len(file.functions)}ä¸ªå‡½æ•°)")
        
        # æœ€éœ€è¦é‡æ„çš„ä»£ç 
        all_complex_functions = []
        for category, data in stats.items():
            if category not in ['æµ‹è¯•ä»£ç ', 'ç¬¬ä¸‰æ–¹/ç”Ÿæˆä»£ç ']:
                all_complex_functions.extend(data['most_complex_functions'])
        
        if all_complex_functions:
            most_complex_overall = sorted(all_complex_functions, key=lambda x: x[0].cognitive_complexity, reverse=True)[:5]
            print(f"\nâš ï¸  éœ€è¦é‡ç‚¹å…³æ³¨çš„å¤æ‚å‡½æ•° (å¯èƒ½éœ€è¦é‡æ„):")
            for i, (func, path) in enumerate(most_complex_overall):
                rel_path = os.path.relpath(path, self.root_dir)
                print(f"   {i+1}. {func.name} (å¤æ‚åº¦:{func.cognitive_complexity}) - {rel_path}")


def main():
    analyzer = GoComplexityAnalyzer()
    stats = analyzer.analyze_project()
    analyzer.print_analysis(stats)


if __name__ == "__main__":
    main() 